{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBRAS-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc, ndimage, signal\n",
    "from sklearn.model_selection  import train_test_split\n",
    "import numpy\n",
    "import numpy as np\n",
    "import random\n",
    "import ntpath\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from keras import optimizers \n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from time import time\n",
    "import time as tm\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "import glob\n",
    "from skimage.util.shape import view_as_blocks\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#USE OF CPU, WHEN IS NOT AVAILABLE THE GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "#!kill PIDnumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 SRM filters for preprocessing and the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################## 30 SRM FILTERS\n",
    "srm_weights = np.load('SRM_Kernels1.npy') \n",
    "biasSRM=numpy.ones(30)\n",
    "print (srm_weights.shape)\n",
    "################################################## TLU ACTIVATION FUNCTION\n",
    "#3xTanH ACTIVATION FUNCTION\n",
    "def Tanh3(x):\n",
    "    tanh3 = tf.keras.activations.tanh(x)*3\n",
    "    return tanh3\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths to save trained models epoch by epoch and final accuracy and loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_log_base = './logs'\n",
    "path_img_base = './images'\n",
    "if not os.path.exists(path_log_base):\n",
    "    os.makedirs(path_log_base)\n",
    "if not os.path.exists(path_img_base):\n",
    "    os.makedirs(path_img_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining different functions to work with the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size, epochs, model_name=\"\"):\n",
    "    start_time = tm.time()\n",
    "    log_dir=path_log_base+\"/\"+model_name+\"_\"+str(datetime.datetime.now().isoformat()[:19].replace(\"T\", \"_\").replace(\":\",\"-\"))\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n",
    "    filepath = log_dir+\"/saved-model-{epoch:03d}-{val_accuracy:.4f}.hdf5\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=False, mode='max')\n",
    "    model.reset_states()\n",
    "    \n",
    "    global lossTEST\n",
    "    global accuracyTEST\n",
    "    global lossTRAIN\n",
    "    global accuracyTRAIN\n",
    "    global lossVALID\n",
    "    global accuracyVALID\n",
    "    lossTEST,accuracyTEST   = model.evaluate(X_test, y_test,verbose=None)\n",
    "    lossTRAIN,accuracyTRAIN = model.evaluate(X_train, y_train,verbose=None)\n",
    "    lossVALID,accuracyVALID = model.evaluate(X_valid, y_valid,verbose=None)\n",
    "\n",
    "    global history\n",
    "    global model_Name\n",
    "    global log_Dir\n",
    "    model_Name = model_name\n",
    "    log_Dir = log_dir\n",
    "    print(\"Starting the training...\")\n",
    "    history=model.fit(X_train, y_train, epochs=epochs, \n",
    "                      callbacks=[tensorboard,checkpoint], \n",
    "                      batch_size=batch_size,validation_data=(X_valid, y_valid),verbose=2)\n",
    "    \n",
    "    metrics = model.evaluate(X_test, y_test, verbose=0)\n",
    "     \n",
    "    TIME = tm.time() - start_time\n",
    "    print(\"Time \"+model_name+\" = %s [seconds]\" % TIME)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(log_dir)\n",
    "    return {k:v for k,v in zip (model.metrics_names, metrics)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Final_Results_Test(PATH_trained_models):\n",
    "    global AccTest\n",
    "    global LossTest\n",
    "    AccTest = []\n",
    "    LossTest= [] \n",
    "    B_accuracy = 0 #B --> Best\n",
    "    for filename in sorted(os.listdir(PATH_trained_models)):\n",
    "        if filename != ('train') and filename != ('validation'):\n",
    "            print(filename)\n",
    "            model = tf.keras.models.load_model(PATH_trained_models+'/'+filename, custom_objects={'Tanh3':Tanh3})\n",
    "            loss,accuracy = model.evaluate(X_test, y_test,verbose=0)\n",
    "            print(f'Loss={loss:.4f} y Accuracy={accuracy:0.4f}'+'\\n')\n",
    "            BandAccTest  = accuracy\n",
    "            BandLossTest = loss\n",
    "            AccTest.append(BandAccTest)    \n",
    "            LossTest.append(BandLossTest)  \n",
    "            \n",
    "            if accuracy > B_accuracy:\n",
    "                B_accuracy = accuracy\n",
    "                B_loss = loss\n",
    "                B_name = filename\n",
    "    \n",
    "    print(\"\\n\\nBest\")\n",
    "    print(B_name)\n",
    "    print(f'Loss={B_loss:.4f} y Accuracy={B_accuracy:0.4f}'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphics(history, AccTest, LossTest, log_Dir, model_Name, lossTEST, lossTRAIN, lossVALID, accuracyTEST, accuracyTRAIN, accuracyVALID):\n",
    "    numbers=AccTest\n",
    "    numbers_sort = sorted(enumerate(numbers), key=itemgetter(1),  reverse=True)\n",
    "    for i in range(int(len(numbers)*(0.05))): #5% total epochs\n",
    "        index, value = numbers_sort[i]\n",
    "        print(\"Test Accuracy {}, epoch:{}\\n\".format(value, index+1))\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    numbers=history.history['accuracy']\n",
    "    numbers_sort = sorted(enumerate(numbers), key=itemgetter(1),  reverse=True)\n",
    "    for i in range(int(len(numbers)*(0.05))): #5% total epochs\n",
    "        index, value = numbers_sort[i]\n",
    "        print(\"Train Accuracy {}, epoch:{}\\n\".format(value, index+1))\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    numbers=history.history['val_accuracy']\n",
    "    numbers_sort = sorted(enumerate(numbers), key=itemgetter(1),  reverse=True)\n",
    "    for i in range(int(len(numbers)*(0.05))): #5% total epochs\n",
    "        index, value = numbers_sort[i]\n",
    "        print(\"Validation Accuracy {}, epoch:{}\\n\".format(value, index+1))\n",
    "\n",
    "    with plt.style.context('seaborn-white'):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.plot(np.concatenate([np.array([accuracyTRAIN]),np.array(history.history['accuracy'])],axis=0))\n",
    "        plt.plot(np.concatenate([np.array([accuracyVALID]),np.array(history.history['val_accuracy'])],axis=0))\n",
    "        plt.plot(np.concatenate([np.array([accuracyTEST]),np.array(AccTest)],axis=0)) #Test\n",
    "        plt.title('Accuracy Vs Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation', 'Test'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(path_img_base+'/Accuracy_GBRAS_Net_'+model_Name+'.eps', format='eps')\n",
    "        plt.savefig(path_img_base+'/Accuracy_GBRAS_Net_'+model_Name+'.svg', format='svg')\n",
    "        plt.savefig(path_img_base+'/Accuracy_GBRAS_Net_'+model_Name+'.pdf', format='pdf')     \n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.plot(np.concatenate([np.array([lossTRAIN]),np.array(history.history['loss'])],axis=0))\n",
    "        plt.plot(np.concatenate([np.array([lossVALID]),np.array(history.history['val_loss'])],axis=0))\n",
    "        plt.plot(np.concatenate([np.array([lossTEST]),np.array(LossTest)],axis=0)) #Test\n",
    "        plt.title('Loss Vs Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation', 'Test'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig(path_img_base+'/Loss_GBRAS_Net_'+model_Name+'.eps', format='eps')\n",
    "        plt.savefig(path_img_base+'/Loss_GBRAS_Net_'+model_Name+'.svg', format='svg')\n",
    "        plt.savefig(path_img_base+'/Loss_GBRAS_Net_'+model_Name+'.pdf', format='pdf') \n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with BOSSbase 1.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading BOSSbase 1.01 dataset for S-UNIWARD 0.4bpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading BOSSbase 1.01 dataset for WOW 0.4bpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=256\n",
    "def load_images(path_pattern):\n",
    "    files=glob.glob(path_pattern)\n",
    "    X=[]\n",
    "    for f in sorted(files):\n",
    "        I = cv2.imread(f, cv2.IMREAD_GRAYSCALE)\n",
    "        patches = view_as_blocks(I, (n, n))\n",
    "        for i in range(patches.shape[0]):\n",
    "            for j in range(patches.shape[1]):\n",
    "                X.append( [ patches[i,j] ] )\n",
    "    X=numpy.array(X)\n",
    "    return X\n",
    "\n",
    "pathc = '../DATABASES/BOSSbase-1.01'\n",
    "paths = '../DATABASES/BOSSbase-1.01/WOW/0.4bpp'\n",
    "\n",
    "Xc_ = load_images(pathc+'/cover/*.pgm') ##COVER IMAGES\n",
    "Xs_ = load_images(paths+'/stego/*.pgm') ##STEGO IMAGES\n",
    "X_  = (numpy.vstack((Xc_, Xs_)))\n",
    "Xt_ = (numpy.hstack(([0]*len(Xc_), [1]*len(Xs_))))\n",
    "Xt_ = np_utils.to_categorical(Xt_, 2)\n",
    "X_  = np.rollaxis(X_,1,4)  #channel axis shifted to last axis\n",
    "\n",
    "print(\"Total image data and labels\",X_.shape,Xt_.shape)\n",
    "#Cover hasta las 10000 ##Train hasta las 4000 ##Valid hasta de las 4000 a las 5000 ##Test de las 5000 a las 10000\n",
    "X_train = np.concatenate([X_[0:4000],X_[10000:14000]],axis=0)\n",
    "X_valid = np.concatenate([X_[4000:5000],X_[14000:15000]],axis=0)\n",
    "X_test  = np.concatenate([X_[5000:10000],X_[15000:20000]],axis=0)\n",
    "y_train = np.concatenate([Xt_[0:4000],Xt_[10000:14000]],axis=0)\n",
    "y_valid = np.concatenate([Xt_[4000:5000],Xt_[14000:15000]],axis=0)\n",
    "y_test  = np.concatenate([Xt_[5000:10000],Xt_[15000:20000]],axis=0)\n",
    "#Controled randomized data for training\n",
    "X_dat0, X_dat1, y_dat0, y_dat1 = train_test_split(X_train, y_train, test_size=0.50, random_state=64) \n",
    "X_train = np.concatenate([X_dat0,X_dat1],axis=0) \n",
    "y_train = np.concatenate([y_dat0,y_dat1],axis=0) \n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GBRAS-Net architecture, and show the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING THE MODEL WITH IMPROVED PREPROCESSING STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBRAS_Net2():\n",
    "    tf.keras.backend.clear_session()\n",
    "    #Inputs\n",
    "    inputs = tf.keras.Input(shape=(256,256,1), name=\"input_1\")\n",
    "    #Layer 1\n",
    "    layers_ty = tf.keras.layers.Conv2D(30, (5,5), weights=[srm_weights,biasSRM], strides=(1,1), padding='same', trainable=False, activation=Tanh3, use_bias=True)(inputs)\n",
    "    layers_tn = tf.keras.layers.Conv2D(30, (5,5), weights=[srm_weights,biasSRM], strides=(1,1), padding='same', trainable=True, activation=Tanh3, use_bias=True)(inputs)\n",
    "  \n",
    "    layers1 = tf.keras.layers.add([layers_ty, layers_tn])\n",
    "    layers1 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers1)\n",
    "    #Layer 2\n",
    "    layers = tf.keras.layers.DepthwiseConv2D(1)(layers1)\n",
    "    layers = tf.keras.layers.SeparableConv2D(30,(3,3), padding='same', activation=None,depth_multiplier=3)(layers) \n",
    "    layers = tf.keras.layers.ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    #Layer 3\n",
    "    layers = tf.keras.layers.DepthwiseConv2D(1)(layers)\n",
    "    layers = tf.keras.layers.SeparableConv2D(30,(3,3), padding='same', activation=None,depth_multiplier=3)(layers) \n",
    "    layers = tf.keras.layers.ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers2 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    skip1 =   tf.keras.layers.Add()([layers1, layers2])\n",
    "    #Layer 4\n",
    "    layers = tf.keras.layers.Conv2D(30, (3,3), strides=(1,1), activation=None, padding='same', kernel_initializer='glorot_uniform')(skip1) \n",
    "    layers = tf.keras.layers.ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    #Layer 5\n",
    "    layers = tf.keras.layers.Conv2D(30, (3,3), strides=(1,1), activation=None, padding='same', kernel_initializer='glorot_uniform')(layers)\n",
    "    layers = tf.keras.layers.ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    #Layer 6\n",
    "    layers = tf.keras.layers.AveragePooling2D((2,2), strides= (2,2))(layers)\n",
    "    #Layer 7\n",
    "    layers = tf.keras.layers.Conv2D(60, (3,3), strides=(1,1), activation=None, padding='same', kernel_initializer='glorot_uniform')(layers) \n",
    "    layers = tf.keras.layers.ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers3 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    #Layer 8\n",
    "    layers = tf.keras.layers.DepthwiseConv2D(1)(layers3)\n",
    "    layers = tf.keras.layers.SeparableConv2D(60,(3,3), padding='same', activation=None,depth_multiplier=3)(layers) \n",
    "    layers = tf.keras.layers.ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    #Layer 9\n",
    "    layers = tf.keras.layers.DepthwiseConv2D(1)(layers)\n",
    "    layers = tf.keras.layers.SeparableConv2D(60,(3,3), padding='same', activation=None,depth_multiplier=3)(layers) \n",
    "    layers = tf.keras.layers.ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers4 = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    skip2 =   tf.keras.layers.Add()([layers3, layers4])\n",
    "    #Layer 10\n",
    "    layers = tf.keras.layers.Conv2D(60, (3,3), strides=(1,1), activation=None, padding='same', kernel_initializer='glorot_uniform')(skip2) \n",
    "    layers = tf.keras.layers.ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    #Layer 11\n",
    "    layers = tf.keras.layers.AveragePooling2D((2,2), strides= (2,2))(layers)\n",
    "    #Layer 12\n",
    "    layers = tf.keras.layers.Conv2D(60, (3,3), strides=(1,1), activation=None, padding='same', kernel_initializer='glorot_uniform')(layers) \n",
    "    layers = tf.keras.layers.ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    #Layer 13\n",
    "    layers = tf.keras.layers.AveragePooling2D((2,2), strides= (2,2))(layers)\n",
    "    #Layer 14\n",
    "    layers = tf.keras.layers.Conv2D(60, (3,3), strides=(1,1), activation=None, padding='same', kernel_initializer='glorot_uniform')(layers) \n",
    "    layers = tf.keras.layers.ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    #Layer 15\n",
    "    layers = tf.keras.layers.AveragePooling2D((2,2), strides= (2,2))(layers)\n",
    "    #Layer 16\n",
    "    layers = tf.keras.layers.Conv2D(30, (1,1), strides=(1,1), activation=None, padding='same', kernel_initializer='glorot_uniform')(layers) \n",
    "    layers = tf.keras.layers.ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    #Layer 17\n",
    "    layers = tf.keras.layers.Conv2D(2, (1,1), strides=(1,1), activation=None, padding='same', kernel_initializer='glorot_uniform')(layers) \n",
    "    layers = tf.keras.layers.ReLU(negative_slope=0.1, threshold=0)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    #Layer 18\n",
    "    layers = tf.keras.layers.GlobalAveragePooling2D(data_format=\"channels_last\")(layers)\n",
    "    #Layer 19\n",
    "    predictions = tf.keras.layers.Softmax(axis=1)(layers)\n",
    "    #Model generation\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "    #Optimizer\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    print (\"Model GBRAS-Net Generated\")\n",
    "    #Model compilation\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model2 = GBRAS_Net2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model2, show_shapes=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GBRAS-Net architecture WOW 0.4bpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(model2, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size=32, epochs=100, model_name=\"model_GBRAS-Net2_WOW_04bpp\") \n",
    "#WOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Final_Results_Test(log_Dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GRAPHICS OF THE THREE (TRAIN, TEST AND VALIDATION) CURVES\n",
    "graphics(history, AccTest, LossTest, log_Dir, model_Name, lossTEST, lossTRAIN, lossVALID, accuracyTEST, accuracyTRAIN, accuracyVALID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
